{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldata_train = 5000\n",
    "df_train = pd.read_csv('input/train.csv')\n",
    "df_train_orig = df_train[:ldata_train]\n",
    "df_train = df_train[:ldata_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-Prépocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remplacement de toutes les balises math par un string\n",
    "df_train['question1'] = [re.sub(r\"\\[math\\].*?\\[/math\\]\", \"math\", question) for question in df_train['question1']]\n",
    "df_train['question2'] = [re.sub(r\"\\[math\\].*?\\[/math\\]\", \"math\", question) for question in df_train['question2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Suppression de tous les charactères pas latin\n",
    "df_train['question1'] = [re.sub(r\"[^\\x00-\\x7F\\x80-\\xFF\\u0100-\\u017F\\u0180-\\u024F\\u1E00-\\u1EFF]\", u\"\", question) for question in df_train['question1']]\n",
    "df_train['question2'] = [re.sub(r\"[^\\x00-\\x7F\\x80-\\xFF\\u0100-\\u017F\\u0180-\\u024F\\u1E00-\\u1EFF]\", u\"\", question) for question in df_train['question2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Suppression de tous les signes de ponctuation et mise en minuscule de tous les caractères\n",
    "df_train['question1'] = [re.sub(r'[^\\w\\s]','',question.lower()) for question in df_train['question1']]\n",
    "df_train['question2'] = [re.sub(r'[^\\w\\s]','',question.lower()) for question in df_train['question2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tokenisation de chaque mot\n",
    "df_train['question1'] = [nltk.word_tokenize(question) for question in df_train['question1']]\n",
    "df_train['question2'] = [nltk.word_tokenize(question) for question in df_train['question2']]\n",
    "df_train_for_pos_features = df_train.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Suppression des stop words\n",
    "df_train['question1'] = [([word for word in question if word not in stopwords.words('english')]) for question in df_train['question1']]\n",
    "df_train['question2'] = [([word for word in question if word not in stopwords.words('english')]) for question in df_train['question2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-Construction des features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fonction renvoyant le nombre de token en commun entre la question 1 et 2 d'une ligne\n",
    "def word_match_share(row):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in row['question1']:\n",
    "        q1words[word] = 1\n",
    "    for word in row['question2']:\n",
    "        q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        return 0\n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
    "    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Application de la fonction word_match_share à notre dataset\n",
    "train_word_match = df_train.apply(word_match_share, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Application de la fonction word_match_share à notre dataset ayant subit un steaming\n",
    "porter = nltk.PorterStemmer()\n",
    "df_train['question1'] = [([porter.stem(word) for word in question]) for question in df_train['question1']]\n",
    "df_train['question2'] = [([porter.stem(word) for word in question]) for question in df_train['question2']]\n",
    "train_word_match_stemmed = df_train.apply(word_match_share, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fonction renvoyant la différence de longueur entre la question 1 et 2 d'une ligne\n",
    "def sentence_length_diff(row):\n",
    "    return abs(len(row['question1']) - len (row['question2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Application de la fonction sentence_length_diff à notre dataset original avant préprocessing\n",
    "train_diff_length = df_train_orig.apply(sentence_length_diff, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fonction renvoyant le nombre de POS tags en commun entre la question 1 et 2 d'une ligne\n",
    "def postags_share(row):\n",
    "    q1tags = {}\n",
    "    q2tags = {}\n",
    "    pos1 = nltk.pos_tag(row['question1'])\n",
    "    pos2 = nltk.pos_tag(row['question2'])\n",
    "    for word in pos1:\n",
    "        q1tags[word[1]] = 1\n",
    "    for word in pos2:\n",
    "        q2tags[word[1]] = 1\n",
    "    if len(q1tags) == 0 or len(q2tags) == 0:\n",
    "        return 0\n",
    "    shared_tags_in_q1 = [w for w in q1tags.keys() if w in q2tags]\n",
    "    shared_tags_in_q2 = [w for w in q2tags.keys() if w in q1tags]\n",
    "    R = (len(shared_tags_in_q1) + len(shared_tags_in_q2))/(len(q1tags) + len(q2tags))\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_match = df_train_for_pos_features.apply(postags_share, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creation de notre vecteur d'entrée\n",
    "x_train = pd.DataFrame()\n",
    "x_train['word_match'] = train_word_match\n",
    "x_train['word_match_stemmed'] = train_word_match_stemmed\n",
    "x_train['diff_length'] = train_diff_length\n",
    "x_train['pos_match'] = train_pos_match\n",
    "y_train = df_train['is_duplicate'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=4242)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-Construction du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import (RandomTreesEmbedding, RandomForestClassifier,\n",
    "                              GradientBoostingClassifier)\n",
    "grd = GradientBoostingClassifier()\n",
    "grd.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import zero_one_loss\n",
    "zero_one_loss(y_valid, grd.predict(x_valid), sample_weight=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
